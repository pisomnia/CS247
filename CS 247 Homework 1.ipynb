{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Answer Sheet\n",
    "\n",
    "__Name__: Dong Wang\n",
    "\n",
    "__UID__: 804363328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Multinomial Naive Bayes\n",
    "\n",
    "Please write down your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood function:\n",
    "$logL(\\Theta)=log \\prod\\limits_d p(x_d,y_d|\\Theta)=\\sum\\limits_d log[p(x_d|y_d)p(y_d)]=\\sum\\limits_d (x_{dn}log \\beta_{y_d n}+log \\pi_{y_d})$   \n",
    "The optimization problem: \n",
    "$$\\max\\limits_\\Theta log L(\\Theta) \\  s.t. \\sum\\limits_j \\pi_j=1 \\  and \\  \\sum\\limits_n \\beta_{jn}=1$$  \n",
    "1.Solve $\\hat{\\pi_j}$:  \n",
    "$$J_1=\\sum\\limits_d log\\pi_{y_d}+\\lambda_1(\\sum\\limits_j \\pi_j-1)=\\sum\\limits_d \\sum\\limits_j \\mathbb{1}(y_d==j) log\\pi_j+\\lambda_1(\\sum\\limits_j \\pi_j-1)$$  \n",
    "$$\\frac{\\partial J_1}{\\partial \\pi_j}=\\sum\\limits_d \\mathbb{1}(y_d==j) \\frac{1}{\\pi_j}+\\lambda_1 =0\\\\  \n",
    "\\Rightarrow \\sum\\limits_d\\sum\\limits_j \\mathbb{1}(y_d==j)+\\lambda_1 \\sum\\limits_j\\pi_j)= \\sum\\limits_d \\sum\\limits_j \\mathbb{1}(y_d==j)+\\lambda_1=0 \\\\ \\Rightarrow \\lambda_1=-\\sum\\limits_j\\sum\\limits_d \\mathbb{1}(y_d==j)$$ \n",
    "So we will have: $\\hat{\\pi_j}=-\\frac{\\sum\\limits_d \\mathbb{1}(y_d==j)}{\\lambda_1}= \\frac{\\sum\\limits_d \\mathbb{1}(y_d==j)}{\\sum\\limits_d \\sum\\limits_j \\mathbb{1}(y_d==j)}=\\frac{\\sum\\limits_d \\mathbb{1}(y_d==j)} {|D|}$  \n",
    "2.Solve $\\hat{\\beta_{jn}}$:  \n",
    "$$J_2=\\sum\\limits_d x_{dn}log \\beta_{y_d n}+\\lambda_2(\\sum\\limits_n \\beta_{jn}-1)=\\sum\\limits_d \\sum\\limits_j \\mathbb{1}(y_d==j) x_{dn}log \\beta_{jn}+\\lambda_2(\\sum\\limits_n \\beta_{jn}-1)$$  \n",
    "$$\\frac{\\partial J_2}{\\partial \\beta_{jn}}=\\frac{\\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}}{\\beta_{jn}}+\\lambda_2=0\\\\ \\Rightarrow \\sum\\limits_{n} \\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}+\\lambda_2 \\sum\\limits_n \\beta_{jn}= \\sum\\limits_n \\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}+\\lambda_2=0 \\\\ \\Rightarrow \\lambda_2=- \\sum\\limits_n \\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}$$  \n",
    "So we will have: $\\hat{\\beta_{jn}}=-\\frac{\\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}}{\\lambda_2}\n",
    "=\\frac{\\sum\\limits_d \\mathbb{1}(y_d==j) x_{dn}}{\\sum\\limits_d \\mathbb{1}(y_d==j)\\sum\\limits_{n^{'}} x_{dn^{'}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Optimization\n",
    "\n",
    "- Implement two versions of optimization (for-loop or matrix) in the Cell 2 below. \n",
    "- Run your experiments by executing Cell 3. \n",
    "- Please do __NOT__ modify Cell 1.\n",
    "- Using existing packages such as `sklearn.linear_model.LogisticRegression` is __NOT__ allowed.\n",
    "- Feel free to comment the example code and write your own.\n",
    "- Necessary changes outside the `TODO` blocks is fine such reformatting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Form Operation\n",
    "### 1. Gradient Vector\n",
    "$$\\frac{\\partial {log L(\\beta)}}{\\partial \\beta}=X^{T}Y-X^{T}\\frac{e^{X\\beta}}{1+e^{X\\beta}}$$\n",
    "\n",
    "### 2. Hessian Matrix\n",
    "$$\\frac{\\partial^2 {log L(\\beta)}}{\\partial \\beta^2}= -(X^{T}p)((1-p)^TX),\\text{where}\\ p=\\frac{e^{X\\beta}}{1+e^{X\\beta}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: helper code. [DO NOT MODIFY]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data loader\n",
    "def load_data():\n",
    "    data = pd.read_csv(\"tic-tac-toe.data\", sep=\",\")\n",
    "    data.rename(columns={'x': 'top left', 'x.1': 'top middle', \n",
    "                         'x.2': 'top right','x.3': 'middle left', \n",
    "                         'o': 'middle middle', 'o.1' : 'middle right', \n",
    "                         'x.4' : 'bottom left', 'o.2' : 'bottom middle', \n",
    "                         'o.3':'bottom right','positive' : 'outcome'},inplace=True)\n",
    "    data_feature = pd.get_dummies(data.iloc[:,0:9])    \n",
    "    data_label = data.iloc[:,9].map({'positive': 1, 'negative': 0})\n",
    "    trainX, testX, trainY, testY = train_test_split(data_feature, data_label, test_size=0.3)\n",
    "    return trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: two versions of optimizer + main function\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "# ==================\n",
    "#  Two Optimizers\n",
    "# ==================\n",
    "\n",
    "def forloop_optimizer(W, X, y, lr):\n",
    "    \"\"\"\n",
    "    Gradient Descent (GD) optimizer implemented with for loop.\n",
    "    \n",
    "    Args:\n",
    "        W - Parameters\n",
    "        X - Features of training batch/instance\n",
    "        y - Label(s) of training batch/instance\n",
    "        lr - Learning rate\n",
    "    \n",
    "    Return:\n",
    "        W_new - Updated W\n",
    "    \"\"\"    \n",
    "    # ========= TODO start ==========\n",
    "    X0 = np.ones((X.shape[0],1))\n",
    "    Xnew = np.hstack((X0,X))\n",
    "    dW=np.zeros(W.shape)\n",
    "    for j in range(len(W)):\n",
    "        for i in range (Xnew.shape[0]):\n",
    "            x=Xnew[i,:]\n",
    "            p=float(np.exp(dW.dot(x)))/float(1+np.exp(dW.dot(x)))\n",
    "            dW[j]+=x[j]*(float(y.values[i])-p)\n",
    "    W_new=W+lr*dW   \n",
    "    # ========= TODO end ============\n",
    "    return W_new\n",
    "\n",
    "\n",
    "def matrix_optimizer(W, X, y, lr):\n",
    "    \"\"\"\n",
    "    Gradient Descent (GD) optimizer implemented with matrix operations.\n",
    "    \n",
    "    See `forloop_optimizer()` for other information.\n",
    "    \"\"\"\n",
    "    # ========= TODO start ==========\n",
    "    X0 = np.ones((X.shape[0],1))\n",
    "    Xnew = np.hstack((X0,X))\n",
    "    dW=np.zeros(W.shape)\n",
    "    P=np.exp(Xnew.dot(W))/(1+np.exp(Xnew.dot(W)))\n",
    "    dW=Xnew.T.dot(y)-Xnew.T.dot(P)\n",
    "    W_new=W+lr*dW\n",
    "    # ========= TODO end ============ \n",
    "    return W_new\n",
    "\n",
    "# =============================\n",
    "#  Training/Testing Functions\n",
    "# =============================\n",
    "\n",
    "def evaluate(W, tstX, tstY):\n",
    "    \"\"\"\n",
    "    Predict on test set and print out the evaluation metrics.\n",
    "    \"\"\"    \n",
    "    # ========= TODO start ==========\n",
    "    X0 = np.ones((tstX.shape[0],1))\n",
    "    tstXnew = np.hstack((X0,tstX))\n",
    "    predY=1*(tstXnew.dot(W)>=0)\n",
    "    # Generate predY\n",
    "    # ========= TODO end ============\n",
    "    print(\"Accuracy score:\")\n",
    "    print(accuracy_score(tstY, predY))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(tstY, predY))\n",
    "    \n",
    "    return accuracy_score(tstY, predY)\n",
    "\n",
    "def logistic_regression(n_epoch,batch_size,lr,optimizer):\n",
    "    \"\"\"\n",
    "    Logistic Regression Classifier\n",
    "    \n",
    "    Args:\n",
    "        n_epoch: number of training epoches\n",
    "        lr: learning rate\n",
    "    \"\"\"   \n",
    "    n_features = 27\n",
    "    trnX, tstX, trnY, tstY = load_data()\n",
    "    W = np.random.rand(n_features + 1)\n",
    "    X_indices = np.arange(trnX.shape[0])\n",
    "    n_batch=int(trnX.shape[0]/batch_size)\n",
    "    # Record the time before training\n",
    "    for epoch in range(n_epoch):\n",
    "        # ========= TODO start ==========\n",
    "        for batch in range(n_batch):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "        # Training Steps\n",
    "        #   Step 1: iterate through batches or samples\n",
    "        #   Step 2: update W by either optimizer\n",
    "        #   Step 3: predict  \n",
    "            batch_indices = np.random.choice(X_indices,batch_size)\n",
    "        # Efficiency\n",
    "        #   Print out time used for training\n",
    "            X_batch = trnX.iloc[batch_indices]\n",
    "            y_batch = trnY.iloc[batch_indices]\n",
    "            if optimizer=='matrix':\n",
    "                W=matrix_optimizer(W, X_batch, y_batch, lr)\n",
    "            if optimizer=='for-loop':\n",
    "                W=forloop_optimizer(W, X_batch, y_batch, lr)\n",
    "        # ========= TODO end ============\n",
    "        if (batch+1) % 5==0:\n",
    "            evaluate(W, tstX, tstY)\n",
    "    accuracy=evaluate(W, tstX, tstY)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.7048611111111112\n",
      "Confusion matrix:\n",
      "[[ 19  85]\n",
      " [  0 184]]\n",
      "Accuracy score:\n",
      "0.9722222222222222\n",
      "Confusion matrix:\n",
      "[[ 93   8]\n",
      " [  0 187]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: execution\n",
    "n_epoch=100\n",
    "batch_size=90\n",
    "lr=0.01\n",
    "# Run with for-loop version optimizer\n",
    "t0 = time.time()\n",
    "accuracy1=logistic_regression(n_epoch,batch_size,lr,'for-loop')  # plugin your configurations\n",
    "time1 = time.time()-t0\n",
    "# Run with matrix-version optimizer\n",
    "t0 = time.time()\n",
    "accuracy2 = logistic_regression(n_epoch,batch_size,lr,'matrix')\n",
    "time2 = time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>for-loop</th>\n",
       "      <td>13.822154</td>\n",
       "      <td>0.704861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix</th>\n",
       "      <td>0.345077</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time  Accuracy\n",
       "for-loop  13.822154  0.704861\n",
       "matrix     0.345077  0.972222"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Result Table\n",
    "data=[[time1,accuracy1],[time2,accuracy2]]\n",
    "result=pd.DataFrame(data)\n",
    "result.columns = ['Time','Accuracy']\n",
    "result.rename(index={0:'for-loop',1:'matrix'}, inplace=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 Poisson Regressions\n",
    "\n",
    "Add your answers in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 $p(y;\\lambda)=e^{-\\lambda}\\frac{\\lambda^y}{y!}=\\frac{1}{y!}exp[(log \\lambda)y-\\lambda]$  \n",
    "compare with the canonical form, we have :$\\eta=log \\lambda,\\ T(y)=y, \\ a(\\eta)=exp(\\eta), \\ b(y)=\\frac{1}{y!}$\n",
    "\n",
    "### 2 $\\eta=x^{T}\\beta, \\mu=\\lambda=exp(\\eta)=exp(x^{T}\\beta)$\n",
    "Possion Regression : $y|x,\\beta \\sim Poission(exp(x^{T}\\beta))$\n",
    "\n",
    "### 3 EX:Estimate the number of traffic accident of one day based on therainfall intensity. Here the response variable is a count number and it can be any integer greater than 0, which follows the possion distribution. So the $\\mu \\in (0,\\infty)$, which can be well explained by $exp(x^T\\beta)$ other than $(x^T\\beta)$ for linear regression and $\\sigma(x^T\\beta)$ for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 Text Classification\n",
    "\n",
    "Please read the description of this problem from the handout before diving into the actual task.\n",
    "\n",
    "The cell(s) below are all yours. Please show the entire process with detailed comments of \n",
    "\n",
    "- loading/preprocessing/featurizing the dataset\n",
    "- building model (you can use sklearn in this task)\n",
    "- evaluating model by numerical metrics or plots\n",
    "\n",
    "Hint:\n",
    "\n",
    "1. Download the files. Take {`alt.atheism`, `soc.religion.christian`, `comp.graphics`, `sci.med`} as an example.\n",
    "```python\n",
    ">>> categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    ">>> from sklearn.datasets import fetch_20newsgroups\n",
    ">>> twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "2. Access the data files\n",
    "```python\n",
    "# Fetch training data 0.\n",
    ">>> twenty_train.data[0]\n",
    "'From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n'\n",
    "# Fetch testing data.\n",
    ">>> twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "3. Access the labels\n",
    "```python\n",
    "# Print out labels\n",
    ">>> twenty_test.target[0]\n",
    "2\n",
    "# Four distinct labels in total\n",
    ">>> set(twenty_test.target)\n",
    "{0, 1, 2, 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing ,loading and featurizing Data\n",
    "def clean_line(t):\n",
    "    return (t.replace('\\n',' ')\n",
    "             .replace('\\r',' ')\n",
    "             .replace('\\t',' ')\n",
    "             .replace('  ',' ')\n",
    "             .strip())\n",
    "\n",
    "def load_and_process_data():\n",
    "    \n",
    "    categories = ['alt.atheism',\n",
    "                 'comp.graphics',\n",
    "                 'comp.os.ms-windows.misc',\n",
    "                 'comp.sys.ibm.pc.hardware',\n",
    "                 'comp.sys.mac.hardware',\n",
    "                 'comp.windows.x',\n",
    "                 'misc.forsale',\n",
    "                 'rec.autos',\n",
    "                 'rec.motorcycles',\n",
    "                 'rec.sport.baseball',\n",
    "                 'rec.sport.hockey',\n",
    "                 'sci.crypt',\n",
    "                 'sci.electronics',\n",
    "                 'sci.med',\n",
    "                 'sci.space',\n",
    "                 'soc.religion.christian',\n",
    "                 'talk.politics.guns',\n",
    "                 'talk.politics.mideast',\n",
    "                 'talk.politics.misc',\n",
    "                 'talk.religion.misc']\n",
    "    #twenty_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "    twenty_train = fetch_20newsgroups(subset='train',shuffle=True,random_state=42)\n",
    "    twenty_test = fetch_20newsgroups(subset='test',shuffle=True,random_state=42)\n",
    "    \n",
    "    X_train = [clean_line(t) for t in twenty_train.data]\n",
    "    X_test = [clean_line(t) for t in twenty_test.data]\n",
    "    y_train=twenty_train.target\n",
    "    y_test=twenty_test.target\n",
    "   \n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_and_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Multinomial Naive Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "predicted_nb = nb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Logistic Regression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "predicted_lr = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shadi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use SVM Classifier\n",
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "predicted_svm = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.52      0.63       319\n",
      "          1       0.81      0.65      0.72       389\n",
      "          2       0.82      0.65      0.73       394\n",
      "          3       0.67      0.78      0.72       392\n",
      "          4       0.86      0.77      0.81       385\n",
      "          5       0.89      0.75      0.82       395\n",
      "          6       0.93      0.69      0.80       390\n",
      "          7       0.85      0.92      0.88       396\n",
      "          8       0.94      0.93      0.93       398\n",
      "          9       0.92      0.90      0.91       397\n",
      "         10       0.89      0.97      0.93       399\n",
      "         11       0.59      0.97      0.74       396\n",
      "         12       0.84      0.60      0.70       393\n",
      "         13       0.92      0.74      0.82       396\n",
      "         14       0.84      0.89      0.87       394\n",
      "         15       0.44      0.98      0.61       398\n",
      "         16       0.64      0.94      0.76       364\n",
      "         17       0.93      0.91      0.92       376\n",
      "         18       0.96      0.42      0.58       310\n",
      "         19       0.97      0.14      0.24       251\n",
      "\n",
      "avg / total       0.82      0.77      0.77      7532\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.74      0.77       319\n",
      "          1       0.69      0.78      0.74       389\n",
      "          2       0.76      0.75      0.75       394\n",
      "          3       0.73      0.72      0.72       392\n",
      "          4       0.81      0.83      0.82       385\n",
      "          5       0.83      0.74      0.78       395\n",
      "          6       0.76      0.90      0.83       390\n",
      "          7       0.91      0.89      0.90       396\n",
      "          8       0.94      0.95      0.94       398\n",
      "          9       0.87      0.93      0.90       397\n",
      "         10       0.94      0.96      0.95       399\n",
      "         11       0.93      0.89      0.91       396\n",
      "         12       0.76      0.78      0.77       393\n",
      "         13       0.89      0.84      0.86       396\n",
      "         14       0.89      0.92      0.91       394\n",
      "         15       0.79      0.93      0.85       398\n",
      "         16       0.71      0.90      0.80       364\n",
      "         17       0.96      0.89      0.92       376\n",
      "         18       0.79      0.58      0.67       310\n",
      "         19       0.83      0.45      0.59       251\n",
      "\n",
      "avg / total       0.83      0.83      0.83      7532\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.76      0.78       319\n",
      "          1       0.77      0.79      0.78       389\n",
      "          2       0.76      0.73      0.74       394\n",
      "          3       0.75      0.74      0.74       392\n",
      "          4       0.80      0.88      0.84       385\n",
      "          5       0.88      0.76      0.81       395\n",
      "          6       0.84      0.92      0.88       390\n",
      "          7       0.93      0.91      0.92       396\n",
      "          8       0.94      0.96      0.95       398\n",
      "          9       0.92      0.95      0.93       397\n",
      "         10       0.96      0.97      0.97       399\n",
      "         11       0.90      0.94      0.92       396\n",
      "         12       0.83      0.78      0.80       393\n",
      "         13       0.89      0.88      0.89       396\n",
      "         14       0.91      0.94      0.93       394\n",
      "         15       0.84      0.93      0.88       398\n",
      "         16       0.74      0.92      0.82       364\n",
      "         17       0.95      0.92      0.93       376\n",
      "         18       0.82      0.62      0.71       310\n",
      "         19       0.75      0.57      0.65       251\n",
      "\n",
      "avg / total       0.85      0.85      0.85      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix Comparison\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(classification_report(y_test, predicted_nb))\n",
    "print(classification_report(y_test, predicted_lr))\n",
    "print(classification_report(y_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NB</th>\n",
       "      <th>LR</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.825531</td>\n",
       "      <td>0.830127</td>\n",
       "      <td>0.849236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.756525</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0.843811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.755754</td>\n",
       "      <td>0.818699</td>\n",
       "      <td>0.844019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NB        LR       SVM\n",
       "Precision  0.825531  0.830127  0.849236\n",
       "Recall     0.756525  0.817749  0.843811\n",
       "F1-score   0.755754  0.818699  0.844019"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy Comparison \n",
    "data=[precision_recall_fscore_support(y_test, predicted_nb,average='macro')[:3],precision_recall_fscore_support(y_test, predicted_lr,average='macro')[:3],precision_recall_fscore_support(y_test, predicted_svm,average='macro')[:3]]\n",
    "data=np.array(data)\n",
    "data.reshape(3,3)\n",
    "result=pd.DataFrame(data.T)\n",
    "result.columns = ['NB','LR','SVM']\n",
    "result.rename(index={0:'Precision',1:'Recall',2:'F1-score'}, inplace=True)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
