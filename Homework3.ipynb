{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1: pLSA Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If $\\theta_{dk}=\\frac{1}{K}$ for every $d$ and $k$, $\\beta_{kw}=\\frac{1}{N}$ for every $k$ and $w$. It means we treat each document and each word as the same, there is no any difference between words and documents. We can learn nothing in the E-M iterative process.\n",
    "\n",
    "Here is the prove: $p(k|w,d)=\\frac{\\beta_{kw}\\theta_{dk}}{\\sum_{k'}\\beta_{k'w}\\theta_{dk'}}=\\frac{1}{K}$,  $\\beta_{kw}=\\frac{\\sum_d p(k|w,d) c(w,d)}{\\sum_{w',d} p(k|w',d) c(w',d)}=\\frac{\\sum_d  c(w,d)}{\\sum_{w',d} c(w',d)}$, $\\theta_{dk}=\\frac{\\sum_w p(k|w,d)c(w,d)}{N_d}=\\frac{\\sum_w c(w,d)}{KN_d}$, we can see that both $\\beta_{kw}$ and $\\theta_{dk}$ are not depending on k which means that the $p(k|w,d)=\\frac{1}{K}$ , no parameter can be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Like the  mideterm problem, $\\beta=\\begin{pmatrix} \n",
    "\\beta_1 & \\beta_2 & \\cdots & \\beta_w \\\\\n",
    "\\vdots  & \\ddots  & \\vdots & \\vdots \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "\\beta_1 & \\beta_2 & \\cdots & \\beta_w \\\\\n",
    "\\end{pmatrix}$, and  $\\theta=\\begin{pmatrix} \n",
    "\\theta_1 & \\cdots & \\theta_K \\\\\n",
    "\\vdots  & \\ddots  & \\vdots \\\\\n",
    "\\theta_1 & \\cdots & \\theta_K \\\\\n",
    "\\end{pmatrix}$.\n",
    "For each topic, the probability of word distribution are the same. And for each document, the probability of topic distribution are the same. The $\\theta$ and $\\beta$ won't be updated in the iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Multinomial Navie Bayes with Dirichlet Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.$$P(x_d,y_d,\\beta|\\alpha)=\\sum\\limits_i^k p(y_d=i) \\prod\\limits_{n=1}^N p(x_{dn}|\\beta_i)p(\\beta_i|\\alpha)\\\\\n",
    "    =\\sum_i^k \\pi_i \\prod_{n=1}^{N}\\beta_{in}^{x_{dn}} p(\\beta_i|\\alpha)\\\\\n",
    "    \\propto \\sum\\limits_{i=1}^k \\pi_i \\prod\\limits_{n=1}^N \\beta_{in}^{x_{dn}}\\beta_i^{\\alpha_i-1}\\\\\n",
    "    =\\sum_{i=1}^k \\pi_i \\prod\\limits_{n=1}^K\\beta_{in}^{x_{dn}+\\alpha_{in}-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $$y^{*}=argmax p(x_d,y)=argmax p(x_d|y)p(y)\\\\\n",
    "   =argmax \\prod\\limits_{n=1}^N \\beta_{yn}^{x_{dn}} \\beta_{y}^{\\alpha_y-1}\\pi_y \\\\\n",
    "   \\propto argmax log(\\prod\\limits_{n=1}{N}\\beta_{yn}^{x_{dn}+\\alpha_{yn}-1 }\\pi_y)\\\\\n",
    "   argmax \\sum\\limits_{n=1}{N}(x_{dn}+\\alpha_{yn}-1)log\\beta_{yn}+log\\pi_y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $$p(\\beta_k|D,\\alpha) \\propto p(D|\\beta_k,\\alpha)p(\\beta_k|\\alpha)\\\\\n",
    "    =\\sum_{d:yd=k} p(x_d|\\beta_k,\\alpha) p(\\beta_k|\\alpha)\\\\\n",
    "    =\\sum_{d:yd=k} \\prod\\limits_{n=1}^N \\beta_{kn}^{x_{dn}}\n",
    "    \\frac{\\Gamma(\\sum_{k'}\\alpha_{k'})}{\\prod_{k'}^{k}\\Gamma(\\alpha_{k'})}\\prod_{k'}^{k}\\beta_{k'}^{\\alpha_{k'}-1}$$\n",
    "  $$E[\\beta_k|D,\\alpha]=\\sum_{d:y_d=k}\\int \\cdots \\int \\beta_k\\prod_{n=1}^{N}\\beta_{kn}^{x_{dn}}\n",
    "     \\frac{\\Gamma(\\sum_{k'}\\alpha_{k'})}{\\prod_{k'}^{k}\\Gamma(\\alpha_{k'})}\\prod_{k'}^{k}\\beta_{k'}^{\\alpha_{k'}-1}\n",
    "     d\\beta_1\\cdots d\\beta_{k-1}\\\\\n",
    "     {\\overset{\\int_{\\beta_k}p(\\beta_k|\\alpha)=1}{\\Longrightarrow}}\\frac{\\sum_{d:y_d=k}x_{dn}+\\alpha_{kn}}{\\sum_{d:y_d=k}\\sum_{n'}x_{dn'}+\\alpha} $$\n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Yes,$X_{ij}$ is symmetric. X_{ij} denotes the number of word j that shows up in the context of word i. In other words, it's the \"distance\" between the word i and word j. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
